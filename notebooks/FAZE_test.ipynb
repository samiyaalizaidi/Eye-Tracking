{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaEEi5nZdbA/eI4a50DF3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiyaalizaidi/Eye-Tracking/blob/main/notebooks/FAZE_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Implementation of **FAZE: Few-Shot Adaptive Gaze Estimation**\n",
        "\n",
        "For more information: [[GitHub](https://github.com/NVlabs/few_shot_gaze.git)] [[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.pdf)]\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Clone the repository"
      ],
      "metadata": {
        "id": "4BVNF5b7OrtJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m49QLvoIOe2v",
        "outputId": "162a674e-55f3-4fd1-b799-153897357815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'few_shot_gaze'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (255/255), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 259 (delta 127), reused 234 (delta 113), pack-reused 4\u001b[K\n",
            "Receiving objects: 100% (259/259), 37.79 MiB | 13.58 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVlabs/few_shot_gaze.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd few_shot_gaze && git submodule update --init --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsftR3VZPIK2",
        "outputId": "4786c24f-6ded-4e69-e5c4-a0cdfbb9140a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submodule 'demo/ext/HRNet-Facial-Landmark-Detection' (https://github.com/HRNet/HRNet-Facial-Landmark-Detection) registered for path 'demo/ext/HRNet-Facial-Landmark-Detection'\n",
            "Submodule 'demo/ext/eos' (https://github.com/patrikhuber/eos) registered for path 'demo/ext/eos'\n",
            "Submodule 'preprocess' (https://github.com/swook/faze_preprocess) registered for path 'preprocess'\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/HRNet-Facial-Landmark-Detection'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos'...\n",
            "Cloning into '/content/few_shot_gaze/preprocess'...\n",
            "Submodule path 'demo/ext/HRNet-Facial-Landmark-Detection': checked out 'acdec19ebb631d0116574f85b367de075ef789f3'\n",
            "Submodule path 'demo/ext/eos': checked out '9ac31018893bc1f3adda7382725c383e1560cfac'\n",
            "Submodule '3rdparty/cereal' (https://github.com/USCiLab/cereal.git) registered for path 'demo/ext/eos/3rdparty/cereal'\n",
            "Submodule '3rdparty/eigen' (https://github.com/eigenteam/eigen-git-mirror.git) registered for path 'demo/ext/eos/3rdparty/eigen'\n",
            "Submodule '3rdparty/eigen3-nnls' (https://github.com/hmatuschek/eigen3-nnls.git) registered for path 'demo/ext/eos/3rdparty/eigen3-nnls'\n",
            "Submodule '3rdparty/glm' (https://github.com/g-truc/glm.git) registered for path 'demo/ext/eos/3rdparty/glm'\n",
            "Submodule '3rdparty/mexplus' (https://github.com/kyamagu/mexplus.git) registered for path 'demo/ext/eos/3rdparty/mexplus'\n",
            "Submodule '3rdparty/nanoflann' (https://github.com/jlblancoc/nanoflann.git) registered for path 'demo/ext/eos/3rdparty/nanoflann'\n",
            "Submodule '3rdparty/pybind11' (https://github.com/pybind/pybind11.git) registered for path 'demo/ext/eos/3rdparty/pybind11'\n",
            "Submodule '3rdparty/toml11' (https://github.com/ToruNiina/toml11.git) registered for path 'demo/ext/eos/3rdparty/toml11'\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/cereal'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/eigen'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/eigen3-nnls'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/glm'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/mexplus'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/nanoflann'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/pybind11'...\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/toml11'...\n",
            "Submodule path 'demo/ext/eos/3rdparty/cereal': checked out '487c3375e11fa2acb92c60e424d5e8fa998db9d6'\n",
            "Submodule path 'demo/ext/eos/3rdparty/eigen': checked out 'dde02fceedfc1ba09d4d4f71a2b5dafcfcb85491'\n",
            "Submodule path 'demo/ext/eos/3rdparty/eigen3-nnls': checked out 'd20add35bcfc9932671cab9ad786b24fd320a592'\n",
            "Submodule path 'demo/ext/eos/3rdparty/glm': checked out '6a1e2ec5d5e79e6d869c947cbdbcbb297bdf9d32'\n",
            "Submodule path 'demo/ext/eos/3rdparty/mexplus': checked out '49457dc4db07cfee279fee550a22b6213c984803'\n",
            "Submodule path 'demo/ext/eos/3rdparty/nanoflann': checked out '206d65a8498a8dfb1910ac8d3de893286fa89e9a'\n",
            "Submodule path 'demo/ext/eos/3rdparty/pybind11': checked out '64205140bdaf02be50d3476bb507e8354a512d04'\n",
            "Submodule 'tools/clang' (https://github.com/wjakob/clang-cindex-python3) registered for path 'demo/ext/eos/3rdparty/pybind11/tools/clang'\n",
            "Cloning into '/content/few_shot_gaze/demo/ext/eos/3rdparty/pybind11/tools/clang'...\n",
            "Submodule path 'demo/ext/eos/3rdparty/pybind11/tools/clang': checked out '6a00cbc4a9b8e68b71caf7f774b3f9c753ae84d5'\n",
            "Submodule path 'demo/ext/eos/3rdparty/toml11': checked out 'b1a55b1331fae95fc06b5a0223dd8c3a539b9258'\n",
            "Submodule path 'preprocess': checked out '5c33caaa1bc271a8d6aad21837e334108f293683'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Pre-Requisites"
      ],
      "metadata": {
        "id": "HjWTFCTXPhFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd few_shot_gaze && bash preprocess/grab_prerequisites.bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B58kvlGWRWf8",
        "outputId": "1deeda60-5589-4467-e9b1-3916d9ee5212"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apex (from -r requirements.txt (line 1))\n",
            "  Downloading apex-0.9.10dev.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.9.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.31.6)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.25.2)\n",
            "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.8.0.76)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.66.4)\n",
            "Collecting tensorboardX (from -r requirements.txt (line 10))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cryptacular (from apex->-r requirements.txt (line 1))\n",
            "  Downloading cryptacular-1.6.2.tar.gz (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy (from apex->-r requirements.txt (line 1))\n",
            "  Downloading zope.sqlalchemy-3.1-py3-none-any.whl (23 kB)\n",
            "Collecting velruse>=1.0.3 (from apex->-r requirements.txt (line 1))\n",
            "  Downloading velruse-1.1.1.tar.gz (709 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.8/709.8 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyramid>1.1.2 (from apex->-r requirements.txt (line 1))\n",
            "  Downloading pyramid-2.0.2-py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyramid_mailer (from apex->-r requirements.txt (line 1))\n",
            "  Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from apex->-r requirements.txt (line 1)) (2.31.0)\n",
            "Collecting wtforms (from apex->-r requirements.txt (line 1))\n",
            "  Downloading wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wtforms-recaptcha (from apex->-r requirements.txt (line 1))\n",
            "  Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->-r requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 4)) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 4)) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r requirements.txt (line 10)) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r requirements.txt (line 10)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->-r requirements.txt (line 4)) (67.7.2)\n",
            "Collecting hupper>=1.5 (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading hupper-1.12.1-py3-none-any.whl (22 kB)\n",
            "Collecting plaster (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading plaster-1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting plaster-pastedeploy (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading plaster_pastedeploy-1.0.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting translationstring>=0.4 (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting venusian>=1.0 (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading venusian-3.1.0-py3-none-any.whl (13 kB)\n",
            "Collecting webob>=1.8.3 (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.deprecation>=3.5.0 (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading zope.deprecation-5.0-py3-none-any.whl (10 kB)\n",
            "Collecting zope.interface>=3.8.0 (from pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading zope.interface-6.4.post2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (2024.6.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from velruse>=1.0.3->apex->-r requirements.txt (line 1)) (1.3.1)\n",
            "Collecting anykeystore (from velruse>=1.0.3->apex->-r requirements.txt (line 1))\n",
            "  Downloading anykeystore-0.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python3-openid (from velruse>=1.0.3->apex->-r requirements.txt (line 1))\n",
            "  Downloading python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pbkdf2 (from cryptacular->apex->-r requirements.txt (line 1))\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 7)) (2.1.5)\n",
            "Collecting repoze.sendmail>=4.1 (from pyramid_mailer->apex->-r requirements.txt (line 1))\n",
            "  Downloading repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transaction (from pyramid_mailer->apex->-r requirements.txt (line 1))\n",
            "  Downloading transaction-4.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex->-r requirements.txt (line 1)) (2.0.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex->-r requirements.txt (line 1)) (3.0.3)\n",
            "Collecting PasteDeploy>=2.0 (from plaster-pastedeploy->pyramid>1.1.2->apex->-r requirements.txt (line 1))\n",
            "  Downloading PasteDeploy-3.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from python3-openid->velruse>=1.0.3->apex->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex->-r requirements.txt (line 1)) (3.2.2)\n",
            "Building wheels for collected packages: apex, velruse, cryptacular, anykeystore, pbkdf2\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-py3-none-any.whl size=46442 sha256=2f4b4f90c5424d8fa5ac3d684e8f66e00f605e59d543e960897be4dca4f2bc42\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/59/9b100fce7ebd989603b3b7a4ca259150da72c9e107fcaa2a30\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-py3-none-any.whl size=50909 sha256=4ba2d5940f7d7f4ff35bfb2b55c012b32d8b337683eafeda374f5ecca821b0b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/f9/a4/fc4ea7b935ee9c58b9bc772cabd94f6a8560f35444097d948d\n",
            "  Building wheel for cryptacular (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.6.2-cp310-cp310-linux_x86_64.whl size=55082 sha256=2271f4b97b448b3bf548fab0d0d94c555f616ec0065c18e49940326a720b4d21\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/6e/09/a7fba517f95b2a6a36bd01b6d4f4679fa7259615a493b64b8f\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-py3-none-any.whl size=16813 sha256=a8fb9842665deb229b453bc0c14b489bb503014e015e8c34235191718d494b51\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/9e/24/35542b7d376b53a6f8426524cc5a3f7998f975037b32d19906\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5083 sha256=4ba841fd6afb2a0fc7c0cdb3f0cd08a5586fc4ff9f8e7dd7049972b0742c870b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/7d/8b/4269ff90fda80497ec59f6ff7d1e1596cb697c1dc8e9bbe320\n",
            "Successfully built apex velruse cryptacular anykeystore pbkdf2\n",
            "Installing collected packages: translationstring, pbkdf2, anykeystore, zope.interface, zope.deprecation, wtforms, webob, venusian, tensorboardX, python3-openid, plaster, PasteDeploy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hupper, cryptacular, wtforms-recaptcha, transaction, plaster-pastedeploy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, zope.sqlalchemy, repoze.sendmail, pyramid, nvidia-cusolver-cu12, velruse, pyramid_mailer, apex\n",
            "\u001b[33m  WARNING: The script hupper is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script qp is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts pdistreport, prequest, proutes, pserve, pshell, ptweens and pviews are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed PasteDeploy-3.1.0 anykeystore-0.2 apex-0.9.10.dev0 cryptacular-1.6.2 hupper-1.12.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pbkdf2-1.3 plaster-1.1.2 plaster-pastedeploy-1.0.1 pyramid-2.0.2 pyramid_mailer-0.15.1 python3-openid-3.2.0 repoze.sendmail-4.4.1 tensorboardX-2.6.2.2 transaction-4.0 translationstring-1.4 velruse-1.1.1 venusian-3.1.0 webob-1.8.7 wtforms-3.1.2 wtforms-recaptcha-0.3.2 zope.deprecation-5.0 zope.interface-6.4.post2 zope.sqlalchemy-3.1\n",
            "--2024-06-23 08:13:03--  https://ait.ethz.ch/projects/2019/faze/downloads/preprocessing/MPIIFaceGaze_supplementary.h5\n",
            "Resolving ait.ethz.ch (ait.ethz.ch)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to ait.ethz.ch (ait.ethz.ch)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-06-23 08:13:04 ERROR 404: Not Found.\n",
            "\n",
            "--2024-06-23 08:13:04--  https://ait.ethz.ch/projects/2019/faze/downloads/preprocessing/GazeCapture_supplementary.h5\n",
            "Resolving ait.ethz.ch (ait.ethz.ch)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to ait.ethz.ch (ait.ethz.ch)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-06-23 08:13:04 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir few_shot_gaze/preprocess/outputs\n",
        "!cd few_shot_gaze/preprocess/outputs && wget -O GazeCapture.h5 https://files.ait.ethz.ch/projects/faze/preprocessing/GazeCapture_supplementary.h5\n",
        "!cd few_shot_gaze/preprocess/outputs && wget -O MPIIGaze.h5 https://files.ait.ethz.ch/projects/faze/preprocessing/MPIIFaceGaze_supplementary.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPyRf6fjKFW4",
        "outputId": "c21e950d-7d7f-4527-c456-3e0b09f8db3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘few_shot_gaze/preprocess/outputs’: File exists\n",
            "--2024-06-23 08:20:35--  https://files.ait.ethz.ch/projects/faze/preprocessing/GazeCapture_supplementary.h5\n",
            "Resolving files.ait.ethz.ch (files.ait.ethz.ch)... 129.132.114.75\n",
            "Connecting to files.ait.ethz.ch (files.ait.ethz.ch)|129.132.114.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 709970152 (677M)\n",
            "Saving to: ‘GazeCapture.h5’\n",
            "\n",
            "GazeCapture.h5      100%[===================>] 677.08M  18.8MB/s    in 39s     \n",
            "\n",
            "2024-06-23 08:21:15 (17.6 MB/s) - ‘GazeCapture.h5’ saved [709970152/709970152]\n",
            "\n",
            "--2024-06-23 08:21:15--  https://files.ait.ethz.ch/projects/faze/preprocessing/MPIIFaceGaze_supplementary.h5\n",
            "Resolving files.ait.ethz.ch (files.ait.ethz.ch)... 129.132.114.75\n",
            "Connecting to files.ait.ethz.ch (files.ait.ethz.ch)|129.132.114.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12846478 (12M)\n",
            "Saving to: ‘MPIIGaze.h5’\n",
            "\n",
            "MPIIGaze.h5         100%[===================>]  12.25M  5.82MB/s    in 2.1s    \n",
            "\n",
            "2024-06-23 08:21:19 (5.82 MB/s) - ‘MPIIGaze.h5’ saved [12846478/12846478]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd few_shot_gaze && pip3 install --user --upgrade -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdeaN3cTPkrg",
        "outputId": "70c69eb9-42de-4232-bdd8-9044fe045516"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apex in /root/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.9.10.dev0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.9.0)\n",
            "Collecting h5py (from -r requirements.txt (line 2))\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.31.6)\n",
            "Collecting imageio (from -r requirements.txt (line 3))\n",
            "  Downloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.25.2)\n",
            "Collecting numpy (from -r requirements.txt (line 5))\n",
            "  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.8.0.76)\n",
            "Collecting opencv_python (from -r requirements.txt (line 6))\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.3.0+cu121)\n",
            "Collecting torch (from -r requirements.txt (line 7))\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.18.0+cu121)\n",
            "Collecting torchvision (from -r requirements.txt (line 8))\n",
            "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.66.4)\n",
            "Requirement already satisfied: tensorboardX in /root/.local/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.6.2.2)\n",
            "Requirement already satisfied: cryptacular in /root/.local/lib/python3.10/site-packages (from apex->-r requirements.txt (line 1)) (1.6.2)\n",
            "Requirement already satisfied: zope.sqlalchemy in /root/.local/lib/python3.10/site-packages (from apex->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: velruse>=1.0.3 in /root/.local/lib/python3.10/site-packages (from apex->-r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: pyramid>1.1.2 in /root/.local/lib/python3.10/site-packages (from apex->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: pyramid-mailer in /root/.local/lib/python3.10/site-packages (from apex->-r requirements.txt (line 1)) (0.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from apex->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: wtforms in /root/.local/lib/python3.10/site-packages (from apex->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: wtforms-recaptcha in /root/.local/lib/python3.10/site-packages (from apex->-r requirements.txt (line 1)) (0.3.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->-r requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 4)) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 4)) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 7)) (12.1.105)\n",
            "Collecting triton==2.3.1 (from torch->-r requirements.txt (line 7))\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /root/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 7)) (12.5.40)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r requirements.txt (line 10)) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r requirements.txt (line 10)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->-r requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: hupper>=1.5 in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: plaster in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: plaster-pastedeploy in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: translationstring>=0.4 in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (1.4)\n",
            "Requirement already satisfied: venusian>=1.0 in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: webob>=1.8.3 in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (1.8.7)\n",
            "Requirement already satisfied: zope.deprecation>=3.5.0 in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (5.0)\n",
            "Requirement already satisfied: zope.interface>=3.8.0 in /root/.local/lib/python3.10/site-packages (from pyramid>1.1.2->apex->-r requirements.txt (line 1)) (6.4.post2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->apex->-r requirements.txt (line 1)) (2024.6.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from velruse>=1.0.3->apex->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: anykeystore in /root/.local/lib/python3.10/site-packages (from velruse>=1.0.3->apex->-r requirements.txt (line 1)) (0.2)\n",
            "Requirement already satisfied: python3-openid in /root/.local/lib/python3.10/site-packages (from velruse>=1.0.3->apex->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: pbkdf2 in /root/.local/lib/python3.10/site-packages (from cryptacular->apex->-r requirements.txt (line 1)) (1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: repoze.sendmail>=4.1 in /root/.local/lib/python3.10/site-packages (from pyramid-mailer->apex->-r requirements.txt (line 1)) (4.4.1)\n",
            "Requirement already satisfied: transaction in /root/.local/lib/python3.10/site-packages (from pyramid-mailer->apex->-r requirements.txt (line 1)) (4.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex->-r requirements.txt (line 1)) (2.0.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: PasteDeploy>=2.0 in /root/.local/lib/python3.10/site-packages (from plaster-pastedeploy->pyramid>1.1.2->apex->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from python3-openid->velruse>=1.0.3->apex->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex->-r requirements.txt (line 1)) (3.2.2)\n",
            "Installing collected packages: triton, numpy, opencv_python, imageio, h5py, torch, torchvision\n",
            "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts imageio_download_bin and imageio_remove_bin are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 imageio-2.34.1 numpy-2.0.0 opencv_python-4.10.0.84 torch-2.3.1 torchvision-0.18.1 triton-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtain the weights"
      ],
      "metadata": {
        "id": "Nmezv3jIPtDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd src/\n",
        "# ! wget -N https://files.ait.ethz.ch/projects/faze/outputs_of_full_train_test_and_plot.zip\n",
        "# ! unzip -o outputs_of_full_train_test_and_plot.zip"
      ],
      "metadata": {
        "id": "i6vNQkrKPPaW",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the all-in-one bash\n"
      ],
      "metadata": {
        "id": "k9VX28h1P6P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's missing strings start and end points in line 17 and 18 of the `full_train_test_and_plot.bash` file."
      ],
      "metadata": {
        "id": "blYuY_adP0nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd few_shot_gaze/src && bash full_train_test_and_plot.bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY8m2GRyP-G0",
        "outputId": "59af4494-15da-44e1-c08d-c3b59c436d2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.local/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "W0623 08:38:11.714000 132345257177728 torch/distributed/run.py:757] \n",
            "W0623 08:38:11.714000 132345257177728 torch/distributed/run.py:757] *****************************************\n",
            "W0623 08:38:11.714000 132345257177728 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0623 08:38:11.714000 132345257177728 torch/distributed/run.py:757] *****************************************\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=1\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=0\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=2\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=3\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=4\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=5\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=6\n",
            "usage: 1_train_dt_ed.py [-h] [--densenet-growthrate DENSENET_GROWTHRATE] [--z-dim-app Z_DIM_APP]\n",
            "                        [--z-dim-gaze Z_DIM_GAZE] [--z-dim-head Z_DIM_HEAD]\n",
            "                        [--decoder-input-c DECODER_INPUT_C] [--normalize-3d-codes]\n",
            "                        [--normalize-3d-codes-axis {1,2,3}]\n",
            "                        [--triplet-loss-type {angular,euclidean}]\n",
            "                        [--triplet-loss-margin TRIPLET_LOSS_MARGIN]\n",
            "                        [--triplet-regularize-d-within] [--all-equal-embeddings]\n",
            "                        [--embedding-consistency-loss-type {angular,euclidean}]\n",
            "                        [--embedding-consistency-loss-warmup-samples EMBEDDING_CONSISTENCY_LOSS_WARMUP_SAMPLES]\n",
            "                        [--backprop-gaze-to-encoder] [--coeff-l1-recon-loss COEFF_L1_RECON_LOSS]\n",
            "                        [--coeff-gaze-loss COEFF_GAZE_LOSS]\n",
            "                        [--coeff-embedding_consistency-loss COEFF_EMBEDDING_CONSISTENCY_LOSS]\n",
            "                        [--pick-exactly-per-person PICK_EXACTLY_PER_PERSON]\n",
            "                        [--pick-at-least-per-person PICK_AT_LEAST_PER_PERSON] [--use-apex]\n",
            "                        [--base-lr LR] [--warmup-period-for-lr LR] [--batch-size N]\n",
            "                        [--decay-interval N] [--decay decay] [--num-training-epochs N]\n",
            "                        [--l2-reg L2_REG] [--print-freq-train N] [--print-freq-test N]\n",
            "                        [--distributed] [--local_rank LOCAL_RANK] [--mpiigaze-file MPIIGAZE_FILE]\n",
            "                        [--gazecapture-file GAZECAPTURE_FILE] [--test-subsample TEST_SUBSAMPLE]\n",
            "                        [--num-data-loaders N] [--use-tensorboard] [--save-path SAVE_PATH]\n",
            "                        [--show-warnings] [--save-freq-images SAVE_FREQ_IMAGES]\n",
            "                        [--save-image-samples SAVE_IMAGE_SAMPLES] [--skip-training]\n",
            "                        [--generate-predictions] [--eval-batch-size N]\n",
            "1_train_dt_ed.py: error: unrecognized arguments: --local-rank=7\n",
            "E0623 08:38:16.804000 132345257177728 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 2) local_rank: 0 (pid: 8088) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/torch/distributed/launch.py\", line 198, in <module>\n",
            "    main()\n",
            "  File \"/root/.local/lib/python3.10/site-packages/torch/distributed/launch.py\", line 194, in main\n",
            "    launch(args)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/torch/distributed/launch.py\", line 179, in launch\n",
            "    run(args)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/torch/distributed/run.py\", line 870, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "1_train_dt_ed.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 2 (pid: 8089)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "[2]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 2 (local_rank: 2)\n",
            "  exitcode  : 2 (pid: 8090)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "[3]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 3 (local_rank: 3)\n",
            "  exitcode  : 2 (pid: 8091)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "[4]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 4 (local_rank: 4)\n",
            "  exitcode  : 2 (pid: 8092)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "[5]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 5 (local_rank: 5)\n",
            "  exitcode  : 2 (pid: 8093)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "[6]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 6 (local_rank: 6)\n",
            "  exitcode  : 2 (pid: 8094)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "[7]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 7 (local_rank: 7)\n",
            "  exitcode  : 2 (pid: 8095)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-06-23_08:38:16\n",
            "  host      : 6dea34b28460\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 8088)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/few_shot_gaze/src/1_train_dt_ed.py\", line 129, in <module>\n",
            "    import moviepy.editor as mpy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/editor.py\", line 48, in <module>\n",
            "    import moviepy.video.fx.all as vfx\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/all/__init__.py\", line 17, in <module>\n",
            "    exec(\"from ..%s import %s\" % (name, name))\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py\", line 7, in <module>\n",
            "    from scipy.ndimage.filters import sobel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/__init__.py\", line 152, in <module>\n",
            "    from ._filters import *  # noqa: F401 F403\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_filters.py\", line 38, in <module>\n",
            "    from . import _nd_image\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py\", line 4, in <module>\n",
            "    from skimage.filter import sobel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/skimage/__init__.py\", line 151, in <module>\n",
            "    from ._shared import geometry\n",
            "  File \"skimage/_shared/geometry.pyx\", line 1, in init skimage._shared.geometry\n",
            "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: _ARRAY_API not found\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/few_shot_gaze/src/1_train_dt_ed.py\", line 129, in <module>\n",
            "    import moviepy.editor as mpy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/editor.py\", line 60, in <module>\n",
            "    from .video.io.sliders import sliders\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/video/io/sliders.py\", line 1, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 131, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n",
            "AttributeError: _ARRAY_API not found\n",
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "2024-06-23 08:38:19,674 DTED(\n",
            "  (encoder): DenseNetEncoder(\n",
            "    (initial): DenseNetInitialLayers(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (block1): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(224, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (trans1): DenseNetTransitionDown(\n",
            "      (composite): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (block2): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(288, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(352, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(352, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (trans2): DenseNetTransitionDown(\n",
            "      (composite): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (block3): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(416, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(416, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(448, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(448, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(480, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (trans3): DenseNetTransitionDown(\n",
            "      (composite): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (block4): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(544, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(544, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(576, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(608, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): Conv2d(608, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): DenseNetDecoder(\n",
            "    (block1): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (trans1): DenseNetTransitionUp(\n",
            "      (norm): InstanceNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "      (conv): ConvTranspose2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (block2): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(160, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(224, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (trans2): DenseNetTransitionUp(\n",
            "      (norm): InstanceNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "      (conv): ConvTranspose2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (block3): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(288, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(352, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(352, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (trans3): DenseNetTransitionUp(\n",
            "      (norm): InstanceNorm2d(416, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "      (conv): ConvTranspose2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (block4): DenseNetBlock(\n",
            "      (compo1): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(416, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(416, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo2): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(448, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(448, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo3): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(480, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (compo4): DenseNetCompositeLayer(\n",
            "        (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        (conv): ConvTranspose2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (last): DenseNetDecoderLastLayers(\n",
            "      (conv1): ConvTranspose2d(544, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (act): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "      (conv2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "      (norm3): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (conv3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_enc): Linear(in_features=640, out_features=118, bias=True)\n",
            "  (fc_dec): Linear(in_features=118, out_features=512, bias=True)\n",
            "  (gaze1): Linear(in_features=6, out_features=64, bias=True)\n",
            "  (gaze2): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n",
            "2024-06-23 08:38:20,697 Initialized optimizer(s)\n",
            "2024-06-23 08:38:20,697 Using 1 GPUs! with DP\n",
            "2024-06-23 08:38:35,071 [gc/train] full set size:           1754166\n",
            "2024-06-23 08:38:35,072 [gc/train] current set size:        1754166\n",
            "2024-06-23 08:38:35,072 [gc/train] num people:                  993\n",
            "2024-06-23 08:38:35,072 [gc/train] mean entries per person:    1766\n",
            "2024-06-23 08:38:35,072 \n",
            "2024-06-23 08:38:35,072   [gc/val] full set size:             81280\n",
            "2024-06-23 08:38:35,072   [gc/val] current set size:          81280\n",
            "2024-06-23 08:38:35,072   [gc/val] num people:                   48\n",
            "2024-06-23 08:38:35,072   [gc/val] mean entries per person:    1693\n",
            "2024-06-23 08:38:35,072 \n",
            "2024-06-23 08:38:35,072  [gc/test] full set size:            251472\n",
            "2024-06-23 08:38:35,072  [gc/test] current set size:          25147\n",
            "2024-06-23 08:38:35,072  [gc/test] num people:                  139\n",
            "2024-06-23 08:38:35,072  [gc/test] mean entries per person:    1809\n",
            "2024-06-23 08:38:35,072 \n",
            "2024-06-23 08:38:35,072      [mpi] full set size:             37577\n",
            "2024-06-23 08:38:35,072      [mpi] current set size:          37577\n",
            "2024-06-23 08:38:35,072      [mpi] num people:                   15\n",
            "2024-06-23 08:38:35,072      [mpi] mean entries per person:    2505\n",
            "2024-06-23 08:38:35,072 \n",
            "2024-06-23 08:38:35,072 Prepared Datasets\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/1_train_dt_ed.py\", line 458, in <module>\n",
            "    retrieved_samples = [dataset[index] for index in indices]\n",
            "  File \"/content/few_shot_gaze/src/1_train_dt_ed.py\", line 458, in <listcomp>\n",
            "    retrieved_samples = [dataset[index] for index in indices]\n",
            "  File \"/content/few_shot_gaze/src/data.py\", line 152, in __getitem__\n",
            "    eyes_a, g_a, h_a = retrieve(group_a, idx_a)\n",
            "  File \"/content/few_shot_gaze/src/data.py\", line 106, in retrieve\n",
            "    eyes = self.preprocess_image(group['pixels'][index, :])\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/group.py\", line 357, in __getitem__\n",
            "    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5o.pyx\", line 241, in h5py.h5o.open\n",
            "KeyError: \"Unable to synchronously open object (object 'pixels' doesn't exist)\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 645, in <module>\n",
            "    meta_train_tasks = Tasks(args.input_dir + '/gc_train_predictions.h5', x_keys=x_keys)\n",
            "  File \"/content/few_shot_gaze/src/2_meta_learning.py\", line 85, in __init__\n",
            "    self.data = h5py.File(hdf_path, 'r')\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
            "  File \"/root/.local/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
            "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs_of_full_train_test_and_plot/gc_train_predictions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/few_shot_gaze/src/3_combine_maml_results.py\", line 15, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 131, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n",
            "AttributeError: _ARRAY_API not found\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/few_shot_gaze/src/3_combine_maml_results.py\", line 15, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 131, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n",
            "ImportError: numpy.core.multiarray failed to import\n"
          ]
        }
      ]
    }
  ]
}